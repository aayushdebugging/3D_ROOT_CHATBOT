{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPW81d9ceY3/WvxT5teU9hA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aayushdebugging/3D_ROOT_CHATBOT/blob/main/3DRoot_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###YOU NEED TO ADD HUGGINGFACE_TOKEN AND OPEN_API_KEY IN SECRETS"
      ],
      "metadata": {
        "id": "qxtKOxJrCwz5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApxRWhCGCVyG"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install \"unstructured[all-docs]\" pillow pydantic lxml matplotlib unstructured\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install poppler-utils\n",
        "!sudo apt-get install libleptonica-dev tesseract-ocr libtesseract-dev python3-pil tesseract-ocr-eng tesseract-ocr-script-latn\n",
        "!pip install unstructured-pytesseract\n",
        "!pip install tesseract-ocr\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Create extracted data directory automatically\n",
        "import os\n",
        "\n",
        "extracted_data_dir = \"/content/extracted_data\"\n",
        "os.makedirs(extracted_data_dir, exist_ok=True)\n",
        "\n",
        "# ✅ Extract PDF elements using Unstructured\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "\n",
        "pdf_path = \"...........\"  # Update this path to your PDF file\n",
        "\n",
        "raw_pdf_elements = partition_pdf(\n",
        "    filename=pdf_path,\n",
        "    strategy=\"hi_res\",\n",
        "    extract_images_in_pdf=True,\n",
        "    extract_image_block_types=[\"Image\", \"Table\"],\n",
        "    extract_image_block_to_payload=False,\n",
        "    extract_image_block_output_dir=extracted_data_dir\n",
        ")"
      ],
      "metadata": {
        "id": "wC17xs9BCYuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Organize extracted data\n",
        "Text = []\n",
        "Table = []\n",
        "Image = []\n",
        "\n",
        "for element in raw_pdf_elements:\n",
        "    element_type = str(type(element))\n",
        "    if \"NarrativeText\" in element_type:\n",
        "        Text.append(str(element))\n",
        "    elif \"Table\" in element_type:\n",
        "        Table.append(str(element))\n",
        "    elif \"Image\" in element_type:\n",
        "        Image.append(str(element))"
      ],
      "metadata": {
        "id": "yG1_Qp5RCjm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "\n",
        "# ✅ Install LangChain and related libraries\n",
        "!pip install langchain_core langchain_openai langchain chromadb\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# ✅ Setup OpenAI API key\n",
        "from google.colab import userdata\n",
        "OPENAI_API_TOKEN = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_TOKEN\n",
        "\n",
        "# ✅ Summarize Tables\n",
        "table_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"You are an assistant tasked with summarizing tables for retrieval. \\\n",
        "    These summaries will be embedded and used to retrieve the raw table elements. \\\n",
        "    Give a concise summary of the table that is well optimized for retrieval. Table:{element}\"\"\"\n",
        ")\n",
        "\n",
        "model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
        "summarize_chain = {\"element\": lambda x: x} | table_prompt | model | StrOutputParser()\n",
        "\n",
        "table_summaries = summarize_chain.batch(Table, {\"max_concurrency\": 5})\n",
        "\n",
        "# ✅ Summarize Text\n",
        "text_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"You are an assistant tasked with summarizing text for retrieval. \\\n",
        "    These summaries will be embedded and used to retrieve the raw text elements. \\\n",
        "    Give a concise summary of the text that is well optimized for retrieval. Text:{element}\"\"\"\n",
        ")\n",
        "\n",
        "text_summaries = summarize_chain.batch(Text, {\"max_concurrency\": 5})\n",
        "\n",
        "# ✅ Summarize Images\n",
        "import base64\n",
        "\n",
        "def encode_image(image_path):\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
        "\n",
        "def image_summarize(img_base64, prompt):\n",
        "    chat = ChatOpenAI(model=\"gpt-4-turbo-2024-04-09\", max_tokens=1024)\n",
        "    msg = chat.invoke(\n",
        "        [\n",
        "            {\n",
        "                \"type\": \"text\",\n",
        "                \"text\": prompt\n",
        "            },\n",
        "            {\n",
        "                \"type\": \"image_url\",\n",
        "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"}\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "    return msg.content\n",
        "\n",
        "def generate_img_summaries(path):\n",
        "    img_base64_list = []\n",
        "    image_summaries = []\n",
        "\n",
        "    prompt = \"Summarize the image concisely for retrieval.\"\n",
        "\n",
        "    for img_file in sorted(os.listdir(path)):\n",
        "        if img_file.endswith(\".jpg\"):\n",
        "            img_path = os.path.join(path, img_file)\n",
        "            base64_image = encode_image(img_path)\n",
        "            img_base64_list.append(base64_image)\n",
        "            image_summaries.append(image_summarize(base64_image, prompt))\n",
        "\n",
        "    return img_base64_list, image_summaries\n",
        "\n",
        "# ✅ Generate image summaries\n",
        "img_base64_list, image_summaries = generate_img_summaries(extracted_data_dir)\n",
        "\n",
        "# ✅ Setup LangChain Retriever\n",
        "!pip install langchain_community\n",
        "\n",
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "import uuid\n",
        "\n",
        "# ✅ Create Multi-Vector Retriever\n",
        "def create_multi_vector_retriever(vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images):\n",
        "    store = InMemoryStore()\n",
        "    id_key = \"doc_id\"\n",
        "\n",
        "    retriever = MultiVectorRetriever(\n",
        "        vectorstore=vectorstore,\n",
        "        docstore=store,\n",
        "        id_key=id_key\n",
        "    )\n",
        "\n",
        "    def add_documents(summaries, contents):\n",
        "        doc_ids = [str(uuid.uuid4()) for _ in contents]\n",
        "        summary_docs = [\n",
        "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
        "            for i, s in enumerate(summaries)\n",
        "        ]\n",
        "        retriever.vectorstore.add_documents(summary_docs)\n",
        "        retriever.docstore.mset(list(zip(doc_ids, contents)))\n",
        "\n",
        "    if text_summaries:\n",
        "        add_documents(text_summaries, texts)\n",
        "    if table_summaries:\n",
        "        add_documents(table_summaries, tables)\n",
        "    if image_summaries:\n",
        "        add_documents(image_summaries, images)\n",
        "\n",
        "    return retriever\n",
        "\n",
        "vectorstore = Chroma(collection_name=\"mm_rag\", embedding_function=OpenAIEmbeddings())\n",
        "\n",
        "retriever = create_multi_vector_retriever(\n",
        "    vectorstore,\n",
        "    text_summaries, Text,\n",
        "    table_summaries, Table,\n",
        "    image_summaries, img_base64_list\n",
        ")\n",
        "\n",
        "# ✅ Create RAG Pipeline\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "\n",
        "def multi_modal_rag_chain(retriever):\n",
        "    model = ChatOpenAI(temperature=0, model=\"gpt-4-turbo-2024-04-09\", max_tokens=1024)\n",
        "\n",
        "    def format_prompt(data_dict):\n",
        "        messages = []\n",
        "        for image in data_dict[\"context\"][\"images\"]:\n",
        "            messages.append({\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"}})\n",
        "        messages.append({\n",
        "            \"type\": \"text\",\n",
        "            \"text\": f\"User question: {data_dict['question']}\\n\\nText:\\n\" + \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
        "        })\n",
        "        return messages\n",
        "\n",
        "    chain = (\n",
        "        {\n",
        "            \"context\": retriever | RunnableLambda(\n",
        "                lambda docs: {\n",
        "                    \"texts\": [doc.page_content for doc in docs if not doc.page_content.startswith(\"/9j\")],\n",
        "                    \"images\": [doc.page_content for doc in docs if doc.page_content.startswith(\"/9j\")]\n",
        "                }\n",
        "            ),\n",
        "            \"question\": RunnablePassthrough(),\n",
        "        }\n",
        "        | RunnableLambda(format_prompt)\n",
        "        | model\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    return chain\n",
        "\n",
        "chain_multimodal_rag = multi_modal_rag_chain(retriever)\n",
        "\n"
      ],
      "metadata": {
        "id": "B-Pj2WxzClfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Query the RAG Pipeline\n",
        "query = \"What information is available about 3D Printing?\"\n",
        "result = chain_multimodal_rag.invoke(query)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "Bh_QV_6mCn1d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}